{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30e50102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 Success! Cleaned data saved at: /Users/miraekang/proyectos/eda/data/processed/ai_impact_jobs_cleaned.csv\n",
      "Final Columns Check: ['job_id', 'posting_year', 'country', 'region', 'city', 'company_name', 'company_size', 'industry', 'job_title', 'seniority_level', 'ai_mentioned', 'ai_keywords', 'ai_intensity_score', 'core_skills', 'ai_skills', 'salary_usd', 'salary_change_vs_prev_year_percent', 'automation_risk_score', 'reskilling_required', 'ai_job_displacement_risk', 'job_description_embedding_cluster', 'industry_ai_adoption_stage']\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Strategic Workforce Analysis: AI Integration vs. Structural Risk (2010-2025)\n",
    "# ## Task 1: Data Preprocessing & Outlier Handling\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- 1. Robust Path Setup ---\n",
    "# Get current working directory (notebooks) and move to parent to find data/\n",
    "BASE_DIR = os.path.dirname(os.getcwd())\n",
    "RAW_DATA_PATH = os.path.join(BASE_DIR, 'data', 'raw', 'ai_impact_jobs_2010_2025.csv')\n",
    "PROCESSED_DIR = os.path.join(BASE_DIR, 'data', 'processed')\n",
    "\n",
    "# Ensure folder exists\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "# 2. Load Dataset\n",
    "df = pd.read_csv(RAW_DATA_PATH)\n",
    "\n",
    "# 3. Data Cleaning (Nulls)\n",
    "df['ai_skills'] = df['ai_skills'].fillna('Not Specified')\n",
    "df['ai_keywords'] = df['ai_keywords'].fillna('None')\n",
    "\n",
    "# 4. NESTED IQR LOGIC (The \"Flower\" of the project)\n",
    "# We calculate IQR for each (Region + Seniority) group to remove local anomalies.\n",
    "cleaned_chunks = []\n",
    "\n",
    "# Get all unique combinations of Region and Seniority\n",
    "for region in df['region'].unique():\n",
    "    for level in df['seniority_level'].unique():\n",
    "        # Create a subset\n",
    "        subset = df[(df['region'] == region) & (df['seniority_level'] == level)]\n",
    "        \n",
    "        if len(subset) > 3: # Only clean if we have enough data\n",
    "            q1 = subset['salary_usd'].quantile(0.25)\n",
    "            q3 = subset['salary_usd'].quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            lower = q1 - 1.5 * iqr\n",
    "            upper = q3 + 1.5 * iqr\n",
    "            # Filter the subset\n",
    "            subset = subset[(subset['salary_usd'] >= lower) & (subset['salary_usd'] <= upper)]\n",
    "        \n",
    "        cleaned_chunks.append(subset)\n",
    "\n",
    "# Combine everything back - Columns are guaranteed to stay\n",
    "df_cleaned = pd.concat(cleaned_chunks).reset_index(drop=True)\n",
    "\n",
    "# 5. Save the Cleaned Dataset\n",
    "CLEANED_FILE_PATH = os.path.join(PROCESSED_DIR, 'ai_impact_jobs_cleaned.csv')\n",
    "df_cleaned.to_csv(CLEANED_FILE_PATH, index=False)\n",
    "\n",
    "print(f\"Task 1 Success! Cleaned data saved at: {CLEANED_FILE_PATH}\")\n",
    "print(f\"Final Columns Check: {df_cleaned.columns.tolist()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (eda-uv)",
   "language": "python",
   "name": "eda-uv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
